{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandrons/RevisitNNWeightInit/blob/main/Experiments_hessian_init.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copyright (c) 2024 Alessandro Temperoni"
      ],
      "metadata": {
        "id": "jJ__bdmOF2Vs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Approximated Hessian Chain Rule\n",
        "\n",
        "The hessian backpropagates is more complicated than for gradient. For a function $L(z)$\n",
        "and parametrization $z=z(w)$ we have\n",
        "\n",
        "$$\n",
        "D^2_{w} L = \\underbrace{D^2_z L \\bullet  D_w z \\bullet D_{w} z}_{\\text{linearization effect}} + \\underbrace{D_w L \\bullet D^2_w z}_{\\text{curvature effect}}\n",
        "$$\n",
        "\n",
        "where $\\bullet$ are tensor product on appropriate axes.\n",
        "\n",
        "We empirically validate (for a theoretical argument see the paper) that for neural networks, under certain conditions, the first term dominates! This helps to very efficiently approximate hessian calculations, e.g. at the initialization.*italicised text*"
      ],
      "metadata": {
        "id": "iG0qV-S1CIJ9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FfDDWOA2nkb"
      },
      "source": [
        "# Empirical Evaluation\n",
        "\n",
        "This is an empirical test that confirms the teory and shows the importace for a correct weighs initialization. Moreover, the Hessian matrix calculation allows us to drive the optimization (step size) and control the gain from the gradients during the backprogation phase."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Different Datasets"
      ],
      "metadata": {
        "id": "hnAa3XhjCXOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(train_inputs, train_labels), _ = mnist.load_data()\n",
        "train_inputs = train_inputs / 255.0"
      ],
      "metadata": {
        "id": "AAskBh4mCduo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cifar100 = tf.keras.datasets.cifar100\n",
        "(train_inputs, train_labels), _ = cifar100.load_data()\n",
        "train_inputs = train_inputs / 255.0"
      ],
      "metadata": {
        "id": "tsXRNQ_7CgwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cifar10 = tf.keras.datasets.cifar10\n",
        "(train_inputs, train_labels), _ = cifar10.load_data()\n",
        "train_inputs = train_inputs / 255.0"
      ],
      "metadata": {
        "id": "m2JqGfnAClml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svhn_cropped = tf.datasets.svhn_cropped\n",
        "(train_inputs, train_labels), _ = svhn_cropped.load_data()\n",
        "train_inputs = train_inputs / 255.0"
      ],
      "metadata": {
        "id": "8TFEEf7jCnyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoxBKlOw3D3d"
      },
      "source": [
        "## Build Model\n",
        "\n",
        "The model is built with Keras A.P.I. and run in Google Colab and it is organized as follows:\n",
        " - input labels\n",
        " - input data images 28x28\n",
        "\n",
        "Then we have a Flatten() layer to switch from input data 28x28 to one dimensional 784 input data vector. After we flatten everything we have\n",
        " - dense1 layer\n",
        " - dense2 layer\n",
        " - dense3 layer\n",
        "\n",
        "Finally we use Categorical Cross-entropy as loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1L-vtmx55j7",
        "outputId": "1dc50580-1f3b-46f9-8d4e-075e95a153cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "def SparseCategoricalCrossentropy(labels,logits):\n",
        "  Z = tf.reduce_logsumexp(logits,axis=-1)\n",
        "  lookup_labels = tf.stack([tf.range(tf.shape(labels)[0]),tf.cast(labels,tf.int32)],1)\n",
        "  true_logits = tf.gather_nd(logits,lookup_labels,batch_dims=0)\n",
        "  return -true_logits + Z\n",
        "\n",
        "def build_network(activation='linear',sd=0.01):\n",
        "\n",
        "  inputs = tf.keras.layers.Input(shape=[28,28],dtype=tf.float32,name='inputs',batch_size=32)\n",
        "  labels = tf.keras.layers.Input(shape=[],dtype=tf.int32,name='labels',batch_size=32)\n",
        "\n",
        "  layer1 = tf.keras.layers.Flatten()\n",
        "  out1 = layer1(inputs)\n",
        "\n",
        "  layer2 = tf.keras.layers.Dense(30,activation=activation,kernel_initializer=tf.keras.initializers.RandomNormal(stddev=sd),name='dense1')\n",
        "  out2 = layer2(out1)\n",
        "\n",
        "  layer3 = tf.keras.layers.Dense(30,activation=activation,name='dense2')\n",
        "  out3 = layer3(out2)\n",
        "\n",
        "  layer4 = tf.keras.layers.Dense(10,activation='linear',name='dense3')\n",
        "  out = layer4(out3)\n",
        "  model = tf.keras.Model(inputs=[inputs,labels], outputs=out)\n",
        "\n",
        "  loss = SparseCategoricalCrossentropy(model.input[1],model.output)\n",
        "  model.add_loss(loss)\n",
        "  sgd = tf.keras.optimizers.SGD(lr=0.01)\n",
        "  model.compile(optimizer=sgd)\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMifwW5q3I8-"
      },
      "source": [
        "## Run Experiment\n",
        "\n",
        "In this section we train the model and do the calculation of the Hessian matrix. Some additional operations are needed to put the H in the right shape. Originally, it would be a 4 dimensional matrix and therefore we will not be able to calculate its norm. Moreover, there is a list of 3 possible standard deviations that will be applied in order to follow epoch by epoch how Hessian matrix and loss function are related to each other.   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9O02tFwy6BYR"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "batch_size = 32\n",
        "sds = [0.00001,0.01,1] # candidate standard deviations to check\n",
        "n_iter = 1000\n",
        "outs = []\n",
        "ends = []\n",
        "for sd in sds:\n",
        "  ## Build fresh model\n",
        "  K.clear_session()\n",
        "  np.random.seed(1234)\n",
        "  model = build_network('relu',sd)\n",
        "  ## choose weights to investigate\n",
        "  g = model.get_layer('dense1').kernel\n",
        "  # By default Keras aggregates the loss by sum, but we wanted by mean so we are sure if the Hessian explodes, it is not because of the aggregation over the batches\n",
        "  loss_agg = tf.reduce_mean(model.total_loss)\n",
        "  # We need to reshape the hessian according to the size of the layer\n",
        "  H = tf.hessians(loss_agg,g)[0]\n",
        "  shape = g.get_shape().as_list()\n",
        "  H = tf.reshape(tf.squeeze(H),[shape[0]*shape[1],shape[0]*shape[1]])\n",
        "  H_norm = tf.norm(H)\n",
        "  # estimate hessians\n",
        "  batch_sample = np.random.randint(0,len(train_labels),size=[1024])\n",
        "  batch_inputs, batch_labels = train_inputs[batch_sample], train_labels[batch_sample]\n",
        "  feed_dict = {model.inputs[0]:batch_inputs,model.inputs[1]:batch_labels}\n",
        "  sess = K.get_session()\n",
        "  h_norm_val = sess.run(H_norm,feed_dict)\n",
        "\n",
        "  # train and estimate loss\n",
        "  for i_epoch in range(n_iter):\n",
        "    np.random.seed(1234)\n",
        "    batch_sample = np.random.randint(0,len(train_labels),size=[batch_size])\n",
        "    batch_inputs, batch_labels = train_inputs[batch_sample], train_labels[batch_sample]\n",
        "    feed_dict = {model.inputs[0]:batch_inputs,model.inputs[1]:batch_labels}\n",
        "    sess = K.get_session()\n",
        "    loss_val = sess.run(loss_agg,feed_dict)\n",
        "    #loss_val = sess.run(loss_agg,feed_dict)\n",
        "    out = (sd,i_epoch,loss_val,h_norm_val)\n",
        "    outs.append(out)\n",
        "    # train the model\n",
        "    model.train_on_batch([batch_inputs,batch_labels])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLobidFj3Mdc"
      },
      "source": [
        "## Summarize\n",
        "\n",
        "Pandas library will diplay the 3 different standard deviation we have been using for the experiment. For big sdt, the Hessian explodes and oscillades from very big values like 100 to very small ones. For small sdt, while the hessian is very small, the gradients are very small too and then we get very small gains."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cM4UsXTYx-Ur"
      },
      "source": [
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "d = pd.DataFrame(outs,columns=['sd','iter','loss','hessian'])\n",
        "d.head()\n",
        "\n",
        "#d.groupby('sd').mean()\n",
        "\n",
        "for sd,mask in d.groupby('sd').groups.items():\n",
        "  #d.loc[mask][['loss']].plot(title='sd=%s'%sd)\n",
        "\n",
        "  tmp = d.loc[mask]\n",
        "\n",
        "  fig, ax1 = plt.subplots()\n",
        "  ax2 = ax1.twinx()\n",
        "\n",
        "  ax1.plot(tmp['loss'],color='blue',label='loss')\n",
        "  ax1.legend(loc=0)\n",
        "  ax2.plot(tmp['hessian'],color='orange',label='hess')\n",
        "  ax2.legend(loc=0)\n",
        "\n",
        "  #out2=tmp.plot(x='iter',y='hessian',color='orange',ax=ax2)\n",
        "\n",
        "  #ax1.legend([out1,out2],['loss','hessian'], loc=0)\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.title('sd=%s'%sd)\n",
        "  plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}